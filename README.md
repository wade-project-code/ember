# Ember æƒ¡æ„è»Ÿé«”åˆ†ææ•™å­¸æŒ‡å—

> ğŸ›¡ï¸ æœ¬å°ˆæ¡ˆå°ˆæ³¨æ–¼é˜²ç¦¦æ€§å®‰å…¨å’Œæ•™è‚²ç ”ç©¶ç›®çš„

## ğŸ“š ç›®éŒ„

- [ç°¡ä»‹](#ç°¡ä»‹)
- [æŠ€è¡“è¦æ ¼](#æŠ€è¡“è¦æ ¼)
- [æŠ€è¡“èƒŒæ™¯](#æŠ€è¡“èƒŒæ™¯)
- [Ember è³‡æ–™é›†æ¦‚è¿°](#ember-è³‡æ–™é›†æ¦‚è¿°)
- [åˆ†ææ–¹æ³•è«–](#åˆ†ææ–¹æ³•è«–)
- [å¯¦å‹™ç¯„ä¾‹](#å¯¦å‹™ç¯„ä¾‹)
- [å·¥å…·èˆ‡æŠ€è¡“](#å·¥å…·èˆ‡æŠ€è¡“)
- [é«˜ç´šåˆ†ææŠ€è¡“](#é«˜ç´šåˆ†ææŠ€è¡“)
- [æŠ€è¡“åƒè€ƒ](#æŠ€è¡“åƒè€ƒ)

## ç°¡ä»‹

Ember è³‡æ–™é›†æ˜¯ç”± Endgameï¼ˆç¾ç‚º Elastic Securityï¼‰é–‹ç™¼çš„å¤§è¦æ¨¡æƒ¡æ„è»Ÿé«”ç ”ç©¶è³‡æ–™é›†ï¼ŒåŒ…å«è¶…é 100 è¬å€‹ PE æª”æ¡ˆçš„ 2351 å€‹ç‰¹å¾µã€‚æœ¬æ•™å­¸æŒ‡å—æ—¨åœ¨å¹«åŠ©å­¸ç”Ÿã€ç ”ç©¶äººå“¡å’Œç¶²è·¯å®‰å…¨å¾æ¥­äººå“¡ç†è§£å’Œæ‡‰ç”¨é€™å€‹å¼·å¤§çš„è³‡æ–™é›†é€²è¡Œé˜²ç¦¦æ€§å®‰å…¨ç ”ç©¶ã€‚

### ç‚ºä»€éº¼é¸æ“‡ Emberï¼Ÿ

- **æ¨™æº–åŒ–**ï¼šæä¾›ä¸€è‡´çš„ç‰¹å¾µæå–å’Œè©•ä¼°åŸºæº–
- **è¦æ¨¡åŒ–**ï¼šå¤§è¦æ¨¡è³‡æ–™é›†æ”¯æ´æ·±åº¦å­¸ç¿’ç ”ç©¶
- **å¯¦ç”¨æ€§**ï¼šåŸºæ–¼çœŸå¯¦ä¸–ç•Œçš„æƒ¡æ„è»Ÿé«”æ¨£æœ¬
- **é–‹æ”¾æ€§**ï¼šé–‹æºä¸”å…è²»æä¾›çµ¦ç ”ç©¶ç¤¾ç¾¤
- **å¯é‡ç¾æ€§**ï¼šæ¨™æº–åŒ–çš„æ–¹æ³•è«–ç¢ºä¿ç ”ç©¶çµæœå¯é‡ç¾

## æŠ€è¡“è¦æ ¼

### æ ¸å¿ƒæŠ€è¡“èƒ½åŠ›

1. **PE æª”æ¡ˆé€†å‘å·¥ç¨‹**
   - æª”æ¡ˆæ ¼å¼è§£æèˆ‡çµæ§‹åˆ†æ
   - åŒ¯å…¥/åŒ¯å‡ºè¡¨åˆ†ææŠ€è¡“
   - ç¯€å€ç‰¹å¾µæå–å’Œç†µå€¼è¨ˆç®—

2. **ç‰¹å¾µå·¥ç¨‹æŠ€è¡“æ£§**
   - 2351ç¶­ç‰¹å¾µå‘é‡æ“ä½œ
   - å¤šç¶­åº¦ç‰¹å¾µç©ºé–“æ˜ å°„
   - çµ±è¨ˆå­¸ç¿’ç‰¹å¾µé¸æ“‡

3. **æ©Ÿå™¨å­¸ç¿’å¯¦ä½œ**
   - LightGBM/XGBoost æ¢¯åº¦æå‡
   - é›†æˆå­¸ç¿’å’Œæ¨¡å‹èåˆ
   - å°æŠ—æ€§æ¨£æœ¬æª¢æ¸¬

4. **åˆ†ææŠ€è¡“æ–¹æ³•**
   - SHAP/LIME æ¨¡å‹è§£é‡‹æ€§
   - ç•°å¸¸æª¢æ¸¬ç®—æ³•å¯¦ä½œ
   - é«˜ç¶­åº¦è³‡æ–™è¦–è¦ºåŒ–

## æŠ€è¡“èƒŒæ™¯

### PE æª”æ¡ˆåˆ†æåŸºç¤

PEï¼ˆPortable Executableï¼‰æ ¼å¼æ˜¯ Windows åŸ·è¡Œæª”çš„æ¨™æº–æ ¼å¼ï¼ŒåŒ…å«è±å¯Œçš„çµæ§‹åŒ–è³‡è¨Šï¼š

```python
# PE æª”æ¡ˆä¸»è¦çµæ§‹
pe_structure = {
    "DOS_Header": "DOS ç›¸å®¹æ€§æ¨™é ­",
    "NT_Headers": {
        "File_Header": "æª”æ¡ˆåŸºæœ¬è³‡è¨Š",
        "Optional_Header": "è¼‰å…¥è³‡è¨Šå’Œç‰¹æ€§"
    },
    "Section_Headers": "ç¯€å€æè¿°è¡¨",
    "Sections": {
        ".text": "ç¨‹å¼ç¢¼æ®µ",
        ".data": "åˆå§‹åŒ–è³‡æ–™æ®µ",
        ".rdata": "åªè®€è³‡æ–™æ®µ",
        ".rsrc": "è³‡æºæ®µ"
    },
    "Import_Table": "åŒ¯å…¥å‡½æ•¸è¡¨",
    "Export_Table": "åŒ¯å‡ºå‡½æ•¸è¡¨"
}
```

### ç‰¹å¾µæå–æ–¹æ³•

Ember æ¡ç”¨å¤šå±¤æ¬¡ç‰¹å¾µæå–æ–¹æ³•ï¼š

1. **çµæ§‹ç‰¹å¾µ**ï¼šæª”æ¡ˆå¤§å°ã€ç¯€å€æ•¸é‡ã€å…¥å£é»ä½ç½®
2. **åŒ¯å…¥ç‰¹å¾µ**ï¼šAPI å‡½æ•¸å‘¼å«ã€DLL ä¾è³´é—œä¿‚
3. **å­—ç¯€çµ±è¨ˆ**ï¼šä½å…ƒçµ„é »ç‡åˆ†ä½ˆã€ç†µå€¼è¨ˆç®—
4. **å­—ä¸²ç‰¹å¾µ**ï¼šå¯åˆ—å°å­—ä¸²çš„çµ±è¨ˆåˆ†æ
5. **PE æ¨™é ­ç‰¹å¾µ**ï¼šç·¨è­¯æ™‚é–“æˆ³ã€æ©Ÿå™¨é¡å‹ã€ç‰¹æ€§æ¨™èªŒ

## Ember è³‡æ–™é›†æ¦‚è¿°

### è³‡æ–™é›†çµæ§‹

```
ember_dataset/
â”œâ”€â”€ train_features.jsonl      # è¨“ç·´ç‰¹å¾µ (800,000 æ¨£æœ¬)
â”œâ”€â”€ train_labels.jsonl        # è¨“ç·´æ¨™ç±¤
â”œâ”€â”€ test_features.jsonl       # æ¸¬è©¦ç‰¹å¾µ (200,000 æ¨£æœ¬)
â”œâ”€â”€ vectorized/              # å‘é‡åŒ–ç‰¹å¾µ
â”‚   â”œâ”€â”€ X_train.npy         # è¨“ç·´ç‰¹å¾µçŸ©é™£
â”‚   â”œâ”€â”€ y_train.npy         # è¨“ç·´æ¨™ç±¤
â”‚   â”œâ”€â”€ X_test.npy          # æ¸¬è©¦ç‰¹å¾µçŸ©é™£
â”‚   â””â”€â”€ metadata.json       # å…ƒè³‡æ–™è³‡è¨Š
â””â”€â”€ README.md               # è³‡æ–™é›†èªªæ˜
```

### ç‰¹å¾µé¡å‹åˆ†æ

#### 1. ä¸€èˆ¬æª”æ¡ˆè³‡è¨Š (10 å€‹ç‰¹å¾µ)
```python
general_features = [
    "size",              # æª”æ¡ˆå¤§å°
    "vsize",             # è™›æ“¬å¤§å°  
    "has_debug",         # æ˜¯å¦åŒ…å«é™¤éŒ¯è³‡è¨Š
    "exports",           # åŒ¯å‡ºå‡½æ•¸æ•¸é‡
    "imports",           # åŒ¯å…¥å‡½æ•¸æ•¸é‡
    "has_relocations",   # æ˜¯å¦æœ‰é‡å®šä½è¡¨
    "has_resources",     # æ˜¯å¦æœ‰è³‡æºæ®µ
    "has_signature",     # æ˜¯å¦æœ‰æ•¸ä½ç°½ç« 
    "has_tls",           # æ˜¯å¦ä½¿ç”¨ TLS
    "symbols"            # ç¬¦è™Ÿæ•¸é‡
]
```

#### 2. æ¨™é ­ç‰¹å¾µ (62 å€‹ç‰¹å¾µ)
```python
header_features = {
    "coff": {
        "timestamp": "ç·¨è­¯æ™‚é–“æˆ³",
        "machine": "ç›®æ¨™æ©Ÿå™¨æ¶æ§‹",
        "characteristics": "æª”æ¡ˆç‰¹æ€§æ¨™èªŒ"
    },
    "optional": {
        "subsystem": "å­ç³»çµ±é¡å‹",
        "dll_characteristics": "DLL ç‰¹æ€§",
        "magic": "PE æ ¼å¼æ¨™è­˜",
        "major_image_version": "æ˜ åƒä¸»ç‰ˆæœ¬è™Ÿ",
        "minor_image_version": "æ˜ åƒæ¬¡ç‰ˆæœ¬è™Ÿ",
        "major_linker_version": "é€£çµå™¨ä¸»ç‰ˆæœ¬è™Ÿ",
        "minor_linker_version": "é€£çµå™¨æ¬¡ç‰ˆæœ¬è™Ÿ",
        "major_operating_system_version": "ä½œæ¥­ç³»çµ±ä¸»ç‰ˆæœ¬è™Ÿ",
        "minor_operating_system_version": "ä½œæ¥­ç³»çµ±æ¬¡ç‰ˆæœ¬è™Ÿ",
        "major_subsystem_version": "å­ç³»çµ±ä¸»ç‰ˆæœ¬è™Ÿ",
        "minor_subsystem_version": "å­ç³»çµ±æ¬¡ç‰ˆæœ¬è™Ÿ",
        "sizeof_code": "ç¨‹å¼ç¢¼æ®µå¤§å°",
        "sizeof_headers": "æ¨™é ­å¤§å°",
        "sizeof_heap_commit": "å †ç©æäº¤å¤§å°"
    }
}
```

#### 3. åŒ¯å…¥ç‰¹å¾µ (1280 å€‹ç‰¹å¾µ)
åŒ¯å…¥ç‰¹å¾µåŸºæ–¼ Windows API å‡½æ•¸çš„å‘¼å«çµ±è¨ˆï¼š

```python
# å¸¸è¦‹çš„æƒ¡æ„è»Ÿé«” API é¡åˆ¥
malware_apis = {
    "Process": ["CreateProcessA", "OpenProcess", "TerminateProcess"],
    "File": ["CreateFileA", "ReadFile", "WriteFile", "DeleteFileA"],
    "Registry": ["RegOpenKeyA", "RegSetValueA", "RegDeleteKeyA"],
    "Network": ["socket", "connect", "send", "recv"],
    "Crypto": ["CryptGenKey", "CryptEncrypt", "CryptDecrypt"],
    "Memory": ["VirtualAlloc", "VirtualProtect", "VirtualFree"],
    "Service": ["OpenSCManagerA", "CreateServiceA", "StartServiceA"],
    "Debug": ["IsDebuggerPresent", "CheckRemoteDebuggerPresent"]
}
```

#### 4. åŒ¯å‡ºç‰¹å¾µ (128 å€‹ç‰¹å¾µ)
```python
export_features = [
    "export_count",      # åŒ¯å‡ºå‡½æ•¸æ•¸é‡
    "export_entropy",    # åŒ¯å‡ºåç¨±ç†µå€¼
    "export_names",      # å…·ååŒ¯å‡ºæ•¸é‡
    "export_ordinals"    # åºè™ŸåŒ¯å‡ºæ•¸é‡
]
```

#### 5. ç¯€å€ç‰¹å¾µ (255 å€‹ç‰¹å¾µ)
```python
section_features = {
    "entry": "å…¥å£é»ç¯€å€",
    "sections": [
        {
            "name": "ç¯€å€åç¨±",
            "size": "ç¯€å€å¤§å°", 
            "entropy": "ç¯€å€ç†µå€¼",
            "vsize": "è™›æ“¬å¤§å°",
            "props": "ç¯€å€å±¬æ€§"
        }
    ]
}
```

#### 6. ä½å…ƒçµ„ç›´æ–¹åœ– (256 å€‹ç‰¹å¾µ)
```python
# è¨ˆç®—ä½å…ƒçµ„é »ç‡åˆ†ä½ˆ
def compute_byte_histogram(file_data):
    histogram = [0] * 256
    for byte in file_data:
        histogram[byte] += 1
    
    # æ­£è¦åŒ–
    total = len(file_data)
    return [count / total for count in histogram]
```

#### 7. ä½å…ƒçµ„ç†µç›´æ–¹åœ– (256 å€‹ç‰¹å¾µ)
```python
import math

def compute_entropy_histogram(file_data, window_size=1024):
    entropies = []
    
    for i in range(0, len(file_data), window_size):
        window = file_data[i:i+window_size]
        if len(window) < window_size:
            break
            
        # è¨ˆç®—è¦–çª—ç†µå€¼
        histogram = [0] * 256
        for byte in window:
            histogram[byte] += 1
        
        entropy = 0
        for count in histogram:
            if count > 0:
                p = count / len(window)
                entropy -= p * math.log2(p)
        
        entropies.append(entropy)
    
    # å°‡ç†µå€¼åˆ†ä½ˆåˆ° 256 å€‹å€é–“
    return create_histogram(entropies, bins=256)
```

#### 8. å­—ä¸²ç‰¹å¾µ (104 å€‹ç‰¹å¾µ)
```python
string_features = {
    "numstrings": "å­—ä¸²ç¸½æ•¸",
    "avlength": "å¹³å‡é•·åº¦",
    "printabledist": "å¯åˆ—å°å­—å…ƒåˆ†ä½ˆ (96 ç¶­)",
    "printables": "å¯åˆ—å°å­—ä¸²æ¯”ä¾‹",
    "entropy": "å­—ä¸²ç†µå€¼",
    "paths": "è·¯å¾‘å­—ä¸²æ•¸é‡",
    "urls": "URL å­—ä¸²æ•¸é‡", 
    "registry": "è¨»å†Šè¡¨é …ç›®æ•¸é‡",
    "MZ": "MZ æ¨™è­˜å‡ºç¾æ¬¡æ•¸"
}
```

### æ¨™ç±¤ç³»çµ±

Ember æ¡ç”¨ä¸‰åˆ†é¡ç³»çµ±ï¼š

```python
labels = {
    0: "è‰¯æ€§è»Ÿé«” (Benign)",
    1: "æƒ¡æ„è»Ÿé«” (Malicious)", 
    -1: "æœªçŸ¥/ä¸ç¢ºå®š (Unknown)"
}

# æ¨™ç±¤åˆ†ä½ˆ (è¨“ç·´é›†)
label_distribution = {
    "benign": 400000,    # 50%
    "malicious": 400000, # 50%
    "unlabeled": 200000  # åœ¨å®Œæ•´è³‡æ–™é›†ä¸­
}
```

## åˆ†ææ–¹æ³•è«–

### 1. éœæ…‹åˆ†ææµç¨‹

```python
import ember
import numpy as np
import pandas as pd

# è¼‰å…¥é è¨“ç·´çš„ç‰¹å¾µæå–å™¨
extractor = ember.FeatureExtractor()

def static_analysis_pipeline(pe_file_path):
    """å®Œæ•´çš„éœæ…‹åˆ†ææµç¨‹"""
    
    # 1. ç‰¹å¾µæå–
    with open(pe_file_path, 'rb') as f:
        raw_features = extractor.feature_vector(f.read())
    
    # 2. ç‰¹å¾µæ­£è¦åŒ–
    features = ember.normalize_features(raw_features)
    
    # 3. è¼‰å…¥é è¨“ç·´æ¨¡å‹
    lgbm_model = ember.load_model()
    
    # 4. é æ¸¬
    prediction = lgbm_model.predict([features])[0]
    
    # 5. çµæœè§£é‡‹
    result = {
        'prediction': prediction,
        'confidence': abs(prediction - 0.5) * 2,
        'classification': 'malicious' if prediction > 0.5 else 'benign',
        'features': features
    }
    
    return result

# ç¯„ä¾‹ä½¿ç”¨
result = static_analysis_pipeline("sample.exe")
print(f"åˆ†é¡çµæœ: {result['classification']}")
print(f"ä¿¡å¿ƒåº¦: {result['confidence']:.3f}")
```

### 2. ç‰¹å¾µé‡è¦æ€§åˆ†æ

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt

def feature_importance_analysis(X_train, y_train):
    """åˆ†æç‰¹å¾µé‡è¦æ€§"""
    
    # è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    
    # ç²å–ç‰¹å¾µé‡è¦æ€§
    importance = rf.feature_importances_
    
    # ç²å– Ember ç‰¹å¾µåç¨±
    feature_names = ember.get_feature_names()
    
    # æ’åºç‰¹å¾µ
    indices = np.argsort(importance)[::-1]
    
    # é¡¯ç¤ºå‰ 20 å€‹é‡è¦ç‰¹å¾µ
    print("å‰ 20 å€‹æœ€é‡è¦ç‰¹å¾µ:")
    for i in range(20):
        idx = indices[i]
        print(f"{i+1:2d}. {feature_names[idx]:30s} ({importance[idx]:.4f})")
    
    # è¦–è¦ºåŒ–ç‰¹å¾µé‡è¦æ€§
    plt.figure(figsize=(12, 8))
    plt.title("ç‰¹å¾µé‡è¦æ€§åˆ†æ")
    plt.bar(range(20), importance[indices[:20]])
    plt.xticks(range(20), [feature_names[i] for i in indices[:20]], rotation=45)
    plt.tight_layout()
    plt.show()
    
    return importance, indices
```

### 3. æ¨¡å‹è¨“ç·´å’Œè©•ä¼°

```python
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
import lightgbm as lgb

def train_ember_model(X_train, y_train, X_test, y_test):
    """è¨“ç·´å’Œè©•ä¼° Ember æ¨¡å‹"""
    
    # 1. è³‡æ–™é è™•ç†
    # ç§»é™¤æœªæ¨™è¨˜çš„æ¨£æœ¬ (æ¨™ç±¤ = -1)
    mask = y_train != -1
    X_train_clean = X_train[mask]
    y_train_clean = y_train[mask]
    
    # 2. LightGBM åƒæ•¸è¨­ç½®
    params = {
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': 'binary_logloss',
        'num_leaves': 1024,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1
    }
    
    # 3. å»ºç«‹è¨“ç·´è³‡æ–™é›†
    train_data = lgb.Dataset(X_train_clean, label=y_train_clean)
    
    # 4. è¨“ç·´æ¨¡å‹
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[train_data],
        early_stopping_rounds=50,
        verbose_eval=False
    )
    
    # 5. æ¸¬è©¦é›†è©•ä¼°
    test_mask = y_test != -1
    X_test_clean = X_test[test_mask]
    y_test_clean = y_test[test_mask]
    
    y_pred_proba = model.predict(X_test_clean, num_iteration=model.best_iteration)
    y_pred = (y_pred_proba > 0.5).astype(int)
    
    # 6. è©•ä¼°çµæœ
    print("æ¨¡å‹è©•ä¼°çµæœ:")
    print(classification_report(y_test_clean, y_pred, 
                              target_names=['Benign', 'Malicious']))
    
    # 7. æ··æ·†çŸ©é™£
    cm = confusion_matrix(y_test_clean, y_pred)
    print("\næ··æ·†çŸ©é™£:")
    print(cm)
    
    # 8. äº¤å‰é©—è­‰
    cv_scores = cross_val_score(
        lgb.LGBMClassifier(**params), 
        X_train_clean, y_train_clean, 
        cv=5, scoring='accuracy'
    )
    print(f"\n5æŠ˜äº¤å‰é©—è­‰æº–ç¢ºç‡: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    
    return model
```

### 4. å°æŠ—æ€§æ¨£æœ¬æª¢æ¸¬

```python
def adversarial_detection(model, X_test, y_test, epsilon=0.01):
    """æª¢æ¸¬å°æŠ—æ€§æ¨£æœ¬æ”»æ“Š"""
    
    # Fast Gradient Sign Method (FGSM) æ”»æ“Š
    def fgsm_attack(x, y, model, epsilon):
        x_tensor = torch.tensor(x, requires_grad=True, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.long)
        
        # å‰å‘å‚³æ’­
        output = model(x_tensor)
        loss = F.cross_entropy(output, y_tensor)
        
        # åå‘å‚³æ’­
        model.zero_grad()
        loss.backward()
        
        # ç”Ÿæˆå°æŠ—æ€§æ¨£æœ¬
        x_adv = x_tensor + epsilon * x_tensor.grad.sign()
        
        return x_adv.detach().numpy()
    
    # æ¸¬è©¦å°æŠ—æ€§æ”»æ“Šçš„æ•ˆæœ
    correct_original = 0
    correct_adversarial = 0
    
    for i in range(len(X_test)):
        x_orig = X_test[i:i+1]
        y_true = y_test[i:i+1]
        
        # åŸå§‹é æ¸¬
        pred_orig = model.predict(x_orig)[0] > 0.5
        if pred_orig == y_true:
            correct_original += 1
        
        # å°æŠ—æ€§æ”»æ“Š
        x_adv = fgsm_attack(x_orig, y_true, model, epsilon)
        pred_adv = model.predict(x_adv)[0] > 0.5
        
        if pred_adv == y_true:
            correct_adversarial += 1
    
    print(f"åŸå§‹æº–ç¢ºç‡: {correct_original / len(X_test):.4f}")
    print(f"å°æŠ—æ€§æ”»æ“Šå¾Œæº–ç¢ºç‡: {correct_adversarial / len(X_test):.4f}")
    
    return correct_original / len(X_test), correct_adversarial / len(X_test)
```

## å¯¦å‹™ç¯„ä¾‹

### ç¯„ä¾‹ 1: æƒ¡æ„è»Ÿé«”å®¶æ—åˆ†é¡

```python
def malware_family_classification():
    """æƒ¡æ„è»Ÿé«”å®¶æ—åˆ†é¡ç¯„ä¾‹"""
    
    # è¼‰å…¥è³‡æ–™å’Œæ¨™ç±¤
    X_train = ember.read_vectorized_features("train")
    y_train = ember.read_labels("train")
    
    # åªé¸æ“‡æƒ¡æ„è»Ÿé«”æ¨£æœ¬
    malicious_mask = y_train == 1
    X_malicious = X_train[malicious_mask]
    
    # å‡è¨­æˆ‘å€‘æœ‰å®¶æ—æ¨™ç±¤ (éœ€è¦é¡å¤–è³‡æ–™)
    # family_labels = load_family_labels()  # è¼‰å…¥å®¶æ—æ¨™ç±¤
    
    # ä½¿ç”¨èšé¡åˆ†ææ¨æ–·å®¶æ—
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    
    # æ¨™æº–åŒ–ç‰¹å¾µ
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_malicious)
    
    # K-means èšé¡
    n_families = 10  # å‡è¨­æœ‰ 10 å€‹å®¶æ—
    kmeans = KMeans(n_clusters=n_families, random_state=42)
    family_clusters = kmeans.fit_predict(X_scaled)
    
    # åˆ†ææ¯å€‹èšé¡çš„ç‰¹å¾µ
    for i in range(n_families):
        cluster_samples = X_malicious[family_clusters == i]
        cluster_size = len(cluster_samples)
        
        print(f"\nå®¶æ— {i+1} (æ¨£æœ¬æ•¸: {cluster_size}):")
        
        # è¨ˆç®—å¹³å‡ç‰¹å¾µ
        mean_features = cluster_samples.mean(axis=0)
        feature_names = ember.get_feature_names()
        
        # é¡¯ç¤ºæœ€çªå‡ºçš„ç‰¹å¾µ
        top_features = np.argsort(mean_features)[-10:]
        for idx in reversed(top_features):
            print(f"  {feature_names[idx]}: {mean_features[idx]:.4f}")
    
    return kmeans, scaler
```

### ç¯„ä¾‹ 2: é›¶æ—¥æƒ¡æ„è»Ÿé«”æª¢æ¸¬

```python
def zero_day_detection():
    """é›¶æ—¥æƒ¡æ„è»Ÿé«”æª¢æ¸¬ç¯„ä¾‹"""
    
    # è¼‰å…¥è¨“ç·´è³‡æ–™
    X_train = ember.read_vectorized_features("train")
    y_train = ember.read_labels("train")
    
    # ç§»é™¤æœªæ¨™è¨˜è³‡æ–™
    labeled_mask = y_train != -1
    X_labeled = X_train[labeled_mask]
    y_labeled = y_train[labeled_mask]
    
    # åˆ†å‰²å·²çŸ¥å’ŒæœªçŸ¥æƒ¡æ„è»Ÿé«”
    # å‡è¨­æˆ‘å€‘æœ‰æ™‚é–“æˆ³è³‡è¨Š
    cutoff_date = "2020-01-01"  # åˆ†ç•Œæ—¥æœŸ
    
    # ä½¿ç”¨ç•°å¸¸æª¢æ¸¬æ–¹æ³•
    from sklearn.ensemble import IsolationForest
    from sklearn.svm import OneClassSVM
    
    # 1. åŸºæ–¼å·²çŸ¥è‰¯æ€§è»Ÿé«”çš„ç•°å¸¸æª¢æ¸¬
    benign_mask = y_labeled == 0
    X_benign = X_labeled[benign_mask]
    
    # è¨“ç·´ One-Class SVM
    ocsvm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.1)
    ocsvm.fit(X_benign)
    
    # 2. åŸºæ–¼éš”é›¢æ£®æ—çš„ç•°å¸¸æª¢æ¸¬
    iso_forest = IsolationForest(contamination=0.1, random_state=42)
    iso_forest.fit(X_labeled)
    
    # æ¸¬è©¦æ–°æ¨£æœ¬
    X_test = ember.read_vectorized_features("test")
    
    # One-Class SVM é æ¸¬
    ocsvm_pred = ocsvm.predict(X_test)
    anomaly_count_svm = np.sum(ocsvm_pred == -1)
    
    # éš”é›¢æ£®æ—é æ¸¬
    iso_pred = iso_forest.predict(X_test)
    anomaly_count_iso = np.sum(iso_pred == -1)
    
    print(f"One-Class SVM æª¢æ¸¬åˆ° {anomaly_count_svm} å€‹ç•°å¸¸æ¨£æœ¬")
    print(f"éš”é›¢æ£®æ—æª¢æ¸¬åˆ° {anomaly_count_iso} å€‹ç•°å¸¸æ¨£æœ¬")
    
    # çµåˆå…©ç¨®æ–¹æ³•
    combined_anomalies = (ocsvm_pred == -1) & (iso_pred == -1)
    high_confidence_anomalies = np.sum(combined_anomalies)
    
    print(f"é«˜ä¿¡å¿ƒåº¦ç•°å¸¸æ¨£æœ¬: {high_confidence_anomalies}")
    
    return ocsvm, iso_forest
```

### ç¯„ä¾‹ 3: ç‰¹å¾µå·¥ç¨‹å’Œé¸æ“‡

```python
def advanced_feature_engineering():
    """é€²éšç‰¹å¾µå·¥ç¨‹ç¯„ä¾‹"""
    
    X_train = ember.read_vectorized_features("train")
    y_train = ember.read_labels("train")
    
    # ç§»é™¤æœªæ¨™è¨˜è³‡æ–™
    labeled_mask = y_train != -1
    X_labeled = X_train[labeled_mask]
    y_labeled = y_train[labeled_mask]
    
    # 1. ç‰¹å¾µç¸®æ”¾
    from sklearn.preprocessing import RobustScaler
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X_labeled)
    
    # 2. ç‰¹å¾µé¸æ“‡
    from sklearn.feature_selection import SelectKBest, mutual_info_classif
    
    # ä½¿ç”¨äº’è³‡è¨Šé¸æ“‡å‰ 1000 å€‹ç‰¹å¾µ
    selector = SelectKBest(score_func=mutual_info_classif, k=1000)
    X_selected = selector.fit_transform(X_scaled, y_labeled)
    
    # 3. ä¸»æˆåˆ†åˆ†æé™ç¶­
    from sklearn.decomposition import PCA
    pca = PCA(n_components=0.95)  # ä¿ç•™ 95% è®Šç•°
    X_pca = pca.fit_transform(X_selected)
    
    print(f"åŸå§‹ç‰¹å¾µæ•¸: {X_labeled.shape[1]}")
    print(f"é¸æ“‡å¾Œç‰¹å¾µæ•¸: {X_selected.shape[1]}")
    print(f"PCA å¾Œç‰¹å¾µæ•¸: {X_pca.shape[1]}")
    print(f"ç´¯ç©è§£é‡‹è®Šç•°æ¯”: {pca.explained_variance_ratio_.cumsum()[-1]:.4f}")
    
    # 4. ç‰¹å¾µé‡è¦æ€§è¦–è¦ºåŒ–
    selected_indices = selector.get_support(indices=True)
    feature_names = ember.get_feature_names()
    selected_features = [feature_names[i] for i in selected_indices]
    scores = selector.scores_[selected_indices]
    
    # ç¹ªè£½ç‰¹å¾µé‡è¦æ€§
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(12, 8))
    top_20 = np.argsort(scores)[-20:]
    plt.barh(range(20), scores[top_20])
    plt.yticks(range(20), [selected_features[i] for i in top_20])
    plt.xlabel('äº’è³‡è¨Šåˆ†æ•¸')
    plt.title('å‰ 20 å€‹æœ€é‡è¦ç‰¹å¾µ')
    plt.tight_layout()
    plt.show()
    
    return scaler, selector, pca, X_pca, y_labeled

# åŸ·è¡Œç‰¹å¾µå·¥ç¨‹
scaler, selector, pca, X_engineered, y_engineered = advanced_feature_engineering()
```

### ç¯„ä¾‹ 4: æ¨¡å‹è§£é‡‹æ€§åˆ†æ

```python
def model_interpretability():
    """æ¨¡å‹è§£é‡‹æ€§åˆ†æç¯„ä¾‹"""
    
    import shap
    import lime
    import lime.tabular
    
    # è¼‰å…¥é è¨“ç·´æ¨¡å‹
    model = ember.load_model()
    X_test = ember.read_vectorized_features("test")
    feature_names = ember.get_feature_names()
    
    # 1. SHAP è§£é‡‹
    print("è¨ˆç®— SHAP å€¼...")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test[:100])  # å‰ 100 å€‹æ¨£æœ¬
    
    # SHAP æ‘˜è¦åœ–
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_test[:100], feature_names=feature_names, show=False)
    plt.title('SHAP ç‰¹å¾µé‡è¦æ€§æ‘˜è¦')
    plt.tight_layout()
    plt.show()
    
    # 2. LIME è§£é‡‹
    print("è¨­ç½® LIME è§£é‡‹å™¨...")
    lime_explainer = lime.tabular.LimeTabularExplainer(
        X_test,
        feature_names=feature_names,
        class_names=['Benign', 'Malicious'],
        mode='classification'
    )
    
    # è§£é‡‹å–®å€‹æ¨£æœ¬
    idx = 0  # è§£é‡‹ç¬¬ä¸€å€‹æ¸¬è©¦æ¨£æœ¬
    lime_exp = lime_explainer.explain_instance(
        X_test[idx], 
        model.predict_proba, 
        num_features=10
    )
    
    # é¡¯ç¤ºè§£é‡‹çµæœ
    print(f"\næ¨£æœ¬ {idx} çš„ LIME è§£é‡‹:")
    for feature, importance in lime_exp.as_list():
        print(f"  {feature}: {importance:.4f}")
    
    # 3. ç‰¹å¾µé‡è¦æ€§ç†±åœ–
    def plot_feature_importance_heatmap():
        # è¨ˆç®—å„ç‰¹å¾µé¡å‹çš„å¹³å‡é‡è¦æ€§
        importance = np.abs(shap_values).mean(axis=0)
        
        # Ember ç‰¹å¾µåˆ†çµ„
        feature_groups = {
            'General': list(range(10)),
            'Header': list(range(10, 72)),
            'Import': list(range(72, 1352)),
            'Export': list(range(1352, 1480)),
            'Section': list(range(1480, 1735)),
            'Byte Histogram': list(range(1735, 1991)),
            'Byte Entropy': list(range(1991, 2247)),
            'String': list(range(2247, 2351))
        }
        
        group_importance = {}
        for group_name, indices in feature_groups.items():
            group_importance[group_name] = importance[indices].mean()
        
        # ç¹ªè£½æ¢å½¢åœ–
        plt.figure(figsize=(10, 6))
        groups = list(group_importance.keys())
        values = list(group_importance.values())
        
        plt.bar(groups, values)
        plt.title('å„ç‰¹å¾µçµ„å¹³å‡é‡è¦æ€§')
        plt.ylabel('SHAP é‡è¦æ€§')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        
        return group_importance
    
    group_imp = plot_feature_importance_heatmap()
    
    return explainer, lime_explainer, shap_values, group_imp

# åŸ·è¡Œè§£é‡‹æ€§åˆ†æ
shap_explainer, lime_explainer, shap_vals, group_importance = model_interpretability()
```

## å·¥å…·èˆ‡æŠ€è¡“

### æ ¸å¿ƒå·¥å…·æ¸…å–®

#### 1. Ember åŸç”Ÿå·¥å…·
```bash
# å®‰è£ Ember
pip install ember-ml

# ä¸‹è¼‰è³‡æ–™é›†
python -c "import ember; ember.download_data()"

# åŸºæœ¬ä½¿ç”¨
python -c "import ember; print(ember.version)"
```

#### 2. éœæ…‹åˆ†æå·¥å…·

```python
# PE æª”æ¡ˆåˆ†æ
import pefile
import pehash

def pe_analysis_tools():
    """PE æª”æ¡ˆåˆ†æå·¥å…·ç¤ºç¯„"""
    
    # ä½¿ç”¨ pefile åˆ†æ
    pe = pefile.PE('sample.exe')
    
    # åŸºæœ¬è³‡è¨Š
    print(f"å…¥å£é»: 0x{pe.OPTIONAL_HEADER.AddressOfEntryPoint:08x}")
    print(f"æ˜ åƒåŸºå€: 0x{pe.OPTIONAL_HEADER.ImageBase:08x}")
    print(f"ç¯€å€æ•¸é‡: {pe.FILE_HEADER.NumberOfSections}")
    
    # åŒ¯å…¥è¡¨åˆ†æ
    if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            print(f"DLL: {entry.dll.decode()}")
            for imp in entry.imports:
                if imp.name:
                    print(f"  å‡½æ•¸: {imp.name.decode()}")
    
    # ç¯€å€åˆ†æ
    for section in pe.sections:
        print(f"ç¯€å€: {section.Name.decode().strip()}")
        print(f"  è™›æ“¬å¤§å°: 0x{section.Misc_VirtualSize:08x}")
        print(f"  åŸå§‹å¤§å°: 0x{section.SizeOfRawData:08x}")
        print(f"  ç†µå€¼: {section.get_entropy():.4f}")
    
    # è¨ˆç®—å„ç¨®é›œæ¹Šå€¼
    hashes = {
        'imphash': pe.get_imphash(),
        'md5': hashlib.md5(pe.__data__).hexdigest(),
        'sha256': hashlib.sha256(pe.__data__).hexdigest()
    }
    
    return pe, hashes

# YARA è¦å‰‡æ•´åˆ
import yara

def yara_scanning():
    """YARA è¦å‰‡æƒæ"""
    
    # è¼‰å…¥ YARA è¦å‰‡
    rules = yara.compile(filepath='malware_rules.yar')
    
    # æƒææª”æ¡ˆ
    matches = rules.match('sample.exe')
    
    for match in matches:
        print(f"è¦å‰‡: {match.rule}")
        print(f"æ¨™ç±¤: {match.tags}")
        print(f"åŒ¹é…å­—ä¸²: {match.strings}")
    
    return matches
```

#### 3. æ©Ÿå™¨å­¸ç¿’å·¥å…·

```python
# é€²éšæ©Ÿå™¨å­¸ç¿’æ¨¡å‹
from sklearn.ensemble import VotingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

def ensemble_learning():
    """é›†æˆå­¸ç¿’æ–¹æ³•"""
    
    # åŸºç¤æ¨¡å‹
    lgb_model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.05)
    xgb_model = XGBClassifier(n_estimators=1000, learning_rate=0.05)
    cat_model = CatBoostClassifier(n_estimators=1000, learning_rate=0.05, verbose=False)
    
    # æŠ•ç¥¨é›†æˆ
    ensemble = VotingClassifier(
        estimators=[
            ('lightgbm', lgb_model),
            ('xgboost', xgb_model),
            ('catboost', cat_model)
        ],
        voting='soft'
    )
    
    return ensemble

# æ·±åº¦å­¸ç¿’æ¨¡å‹
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization

def deep_learning_model():
    """æ·±åº¦å­¸ç¿’æƒ¡æ„è»Ÿé«”æª¢æ¸¬æ¨¡å‹"""
    
    model = Sequential([
        Dense(1024, input_dim=2351, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        
        Dense(128, activation='relu'),
        Dropout(0.2),
        
        Dense(1, activation='sigmoid')
    ])
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model
```

#### 4. è¦–è¦ºåŒ–å·¥å…·

```python
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

def advanced_visualization():
    """é€²éšè¦–è¦ºåŒ–åˆ†æ"""
    
    # è¼‰å…¥è³‡æ–™
    X_train = ember.read_vectorized_features("train")
    y_train = ember.read_labels("train")
    
    # 1. ç‰¹å¾µåˆ†ä½ˆè¦–è¦ºåŒ–
    def plot_feature_distributions():
        # é¸æ“‡éƒ¨åˆ†ç‰¹å¾µé€²è¡Œè¦–è¦ºåŒ–
        feature_indices = [0, 10, 72, 1352, 1480, 1735, 1991, 2247]
        feature_names = ember.get_feature_names()
        
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        axes = axes.ravel()
        
        for i, idx in enumerate(feature_indices):
            benign_values = X_train[y_train == 0, idx]
            malicious_values = X_train[y_train == 1, idx]
            
            axes[i].hist(benign_values, bins=50, alpha=0.7, label='Benign', density=True)
            axes[i].hist(malicious_values, bins=50, alpha=0.7, label='Malicious', density=True)
            axes[i].set_title(feature_names[idx][:30])
            axes[i].legend()
        
        plt.tight_layout()
        plt.show()
    
    # 2. t-SNE é™ç¶­è¦–è¦ºåŒ–
    def plot_tsne():
        from sklearn.manifold import TSNE
        
        # éš¨æ©Ÿé¸æ“‡æ¨£æœ¬ä»¥åŠ å¿«è¨ˆç®—
        sample_size = 10000
        indices = np.random.choice(len(X_train), sample_size, replace=False)
        X_sample = X_train[indices]
        y_sample = y_train[indices]
        
        # ç§»é™¤æœªæ¨™è¨˜æ¨£æœ¬
        labeled_mask = y_sample != -1
        X_labeled = X_sample[labeled_mask]
        y_labeled = y_sample[labeled_mask]
        
        # t-SNE é™ç¶­
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        X_tsne = tsne.fit_transform(X_labeled)
        
        # ç¹ªè£½æ•£é»åœ–
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_labeled, 
                            cmap='coolwarm', alpha=0.6)
        plt.colorbar(scatter, label='Label')
        plt.title('t-SNE è¦–è¦ºåŒ– (ç´…è‰²=æƒ¡æ„è»Ÿé«”, è—è‰²=è‰¯æ€§è»Ÿé«”)')
        plt.xlabel('t-SNE 1')
        plt.ylabel('t-SNE 2')
        plt.show()
    
    # 3. äº’å‹•å¼è¦–è¦ºåŒ–
    def interactive_visualization():
        # ä½¿ç”¨ Plotly å»ºç«‹äº’å‹•å¼åœ–è¡¨
        sample_size = 5000
        indices = np.random.choice(len(X_train), sample_size, replace=False)
        X_sample = X_train[indices]
        y_sample = y_train[indices]
        
        labeled_mask = y_sample != -1
        X_labeled = X_sample[labeled_mask]
        y_labeled = y_sample[labeled_mask]
        
        # é¸æ“‡å‰ä¸‰å€‹ä¸»æˆåˆ†
        from sklearn.decomposition import PCA
        pca = PCA(n_components=3)
        X_pca = pca.fit_transform(X_labeled)
        
        # 3D æ•£é»åœ–
        fig = go.Figure(data=go.Scatter3d(
            x=X_pca[:, 0],
            y=X_pca[:, 1], 
            z=X_pca[:, 2],
            mode='markers',
            marker=dict(
                size=3,
                color=y_labeled,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="æ¨™ç±¤")
            ),
            text=[f"æ¨£æœ¬ {i}: {'æƒ¡æ„' if y_labeled[i] else 'è‰¯æ€§'}" 
                  for i in range(len(y_labeled))],
            hovertemplate='%{text}<br>PC1: %{x}<br>PC2: %{y}<br>PC3: %{z}<extra></extra>'
        ))
        
        fig.update_layout(
            title='Ember è³‡æ–™é›† 3D PCA è¦–è¦ºåŒ–',
            scene=dict(
                xaxis_title='ç¬¬ä¸€ä¸»æˆåˆ†',
                yaxis_title='ç¬¬äºŒä¸»æˆåˆ†',
                zaxis_title='ç¬¬ä¸‰ä¸»æˆåˆ†'
            )
        )
        
        fig.show()
    
    plot_feature_distributions()
    plot_tsne()
    interactive_visualization()

# åŸ·è¡Œè¦–è¦ºåŒ–
advanced_visualization()
```

### è‡ªå‹•åŒ–åˆ†ææµæ°´ç·š

```python
import json
import logging
from datetime import datetime
from pathlib import Path

class EmberAnalysisPipeline:
    """Ember åˆ†æè‡ªå‹•åŒ–æµæ°´ç·š"""
    
    def __init__(self, config_path="config.json"):
        self.config = self.load_config(config_path)
        self.setup_logging()
        self.model = None
        self.scaler = None
        self.feature_selector = None
    
    def load_config(self, config_path):
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒè¨˜éŒ„"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('ember_analysis.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def preprocess_data(self):
        """è³‡æ–™é è™•ç†"""
        self.logger.info("é–‹å§‹è³‡æ–™é è™•ç†...")
        
        # è¼‰å…¥åŸå§‹è³‡æ–™
        X_train = ember.read_vectorized_features("train")
        y_train = ember.read_labels("train")
        X_test = ember.read_vectorized_features("test")
        y_test = ember.read_labels("test")
        
        # ç§»é™¤æœªæ¨™è¨˜è³‡æ–™
        train_mask = y_train != -1
        test_mask = y_test != -1
        
        self.X_train = X_train[train_mask]
        self.y_train = y_train[train_mask]
        self.X_test = X_test[test_mask]
        self.y_test = y_test[test_mask]
        
        self.logger.info(f"è¨“ç·´é›†å¤§å°: {self.X_train.shape}")
        self.logger.info(f"æ¸¬è©¦é›†å¤§å°: {self.X_test.shape}")
    
    def feature_engineering(self):
        """ç‰¹å¾µå·¥ç¨‹"""
        self.logger.info("é–‹å§‹ç‰¹å¾µå·¥ç¨‹...")
        
        # ç‰¹å¾µç¸®æ”¾
        from sklearn.preprocessing import RobustScaler
        self.scaler = RobustScaler()
        X_train_scaled = self.scaler.fit_transform(self.X_train)
        X_test_scaled = self.scaler.transform(self.X_test)
        
        # ç‰¹å¾µé¸æ“‡
        from sklearn.feature_selection import SelectKBest, f_classif
        self.feature_selector = SelectKBest(
            score_func=f_classif, 
            k=self.config['feature_selection']['k']
        )
        
        self.X_train_processed = self.feature_selector.fit_transform(
            X_train_scaled, self.y_train
        )
        self.X_test_processed = self.feature_selector.transform(X_test_scaled)
        
        self.logger.info(f"ç‰¹å¾µé¸æ“‡å¾Œç¶­åº¦: {self.X_train_processed.shape[1]}")
    
    def train_model(self):
        """è¨“ç·´æ¨¡å‹"""
        self.logger.info("é–‹å§‹æ¨¡å‹è¨“ç·´...")
        
        model_type = self.config['model']['type']
        
        if model_type == 'lightgbm':
            import lightgbm as lgb
            
            params = self.config['model']['lightgbm_params']
            train_data = lgb.Dataset(self.X_train_processed, label=self.y_train)
            
            self.model = lgb.train(
                params,
                train_data,
                num_boost_round=self.config['model']['num_boost_round'],
                valid_sets=[train_data],
                early_stopping_rounds=50,
                verbose_eval=False
            )
            
        elif model_type == 'ensemble':
            from sklearn.ensemble import VotingClassifier
            import lightgbm as lgb
            from xgboost import XGBClassifier
            
            lgb_model = lgb.LGBMClassifier(**self.config['model']['lightgbm_params'])
            xgb_model = XGBClassifier(**self.config['model']['xgboost_params'])
            
            self.model = VotingClassifier(
                estimators=[
                    ('lightgbm', lgb_model),
                    ('xgboost', xgb_model)
                ],
                voting='soft'
            )
            
            self.model.fit(self.X_train_processed, self.y_train)
        
        self.logger.info("æ¨¡å‹è¨“ç·´å®Œæˆ")
    
    def evaluate_model(self):
        """æ¨¡å‹è©•ä¼°"""
        self.logger.info("é–‹å§‹æ¨¡å‹è©•ä¼°...")
        
        from sklearn.metrics import classification_report, roc_auc_score
        
        # é æ¸¬
        if hasattr(self.model, 'predict_proba'):
            y_pred_proba = self.model.predict_proba(self.X_test_processed)[:, 1]
        else:
            y_pred_proba = self.model.predict(
                self.X_test_processed, 
                num_iteration=self.model.best_iteration
            )
        
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # è¨ˆç®—æŒ‡æ¨™
        auc_score = roc_auc_score(self.y_test, y_pred_proba)
        report = classification_report(
            self.y_test, y_pred, 
            target_names=['Benign', 'Malicious'],
            output_dict=True
        )
        
        # è¨˜éŒ„çµæœ
        self.logger.info(f"AUC åˆ†æ•¸: {auc_score:.4f}")
        self.logger.info(f"æº–ç¢ºç‡: {report['accuracy']:.4f}")
        self.logger.info(f"ç²¾ç¢ºç‡: {report['Malicious']['precision']:.4f}")
        self.logger.info(f"å¬å›ç‡: {report['Malicious']['recall']:.4f}")
        
        # ä¿å­˜çµæœ
        results = {
            'timestamp': datetime.now().isoformat(),
            'auc_score': auc_score,
            'classification_report': report,
            'config': self.config
        }
        
        with open('evaluation_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return results
    
    def save_model(self, model_path="trained_model"):
        """ä¿å­˜æ¨¡å‹"""
        import joblib
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler, 
            'feature_selector': self.feature_selector,
            'config': self.config
        }
        
        joblib.dump(model_data, f"{model_path}.pkl")
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³ {model_path}.pkl")
    
    def run_pipeline(self):
        """åŸ·è¡Œå®Œæ•´åˆ†ææµæ°´ç·š"""
        try:
            self.preprocess_data()
            self.feature_engineering()
            self.train_model()
            results = self.evaluate_model()
            self.save_model()
            
            self.logger.info("åˆ†ææµæ°´ç·šåŸ·è¡Œå®Œæˆï¼")
            return results
            
        except Exception as e:
            self.logger.error(f"æµæ°´ç·šåŸ·è¡Œå‡ºéŒ¯: {str(e)}")
            raise

# é…ç½®æ–‡ä»¶ç¯„ä¾‹
config_example = {
    "feature_selection": {
        "k": 1000
    },
    "model": {
        "type": "lightgbm",
        "num_boost_round": 1000,
        "lightgbm_params": {
            "boosting_type": "gbdt",
            "objective": "binary",
            "metric": "binary_logloss",
            "num_leaves": 1024,
            "learning_rate": 0.05,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "verbose": -1
        },
        "xgboost_params": {
            "n_estimators": 1000,
            "learning_rate": 0.05,
            "max_depth": 8,
            "random_state": 42
        }
    }
}

# ä¿å­˜é…ç½®æ–‡ä»¶
with open('config.json', 'w', encoding='utf-8') as f:
    json.dump(config_example, f, ensure_ascii=False, indent=2)

# åŸ·è¡Œæµæ°´ç·š
pipeline = EmberAnalysisPipeline()
results = pipeline.run_pipeline()
```

## é«˜ç´šåˆ†ææŠ€è¡“

### 1. è©•ä¼°æŒ‡æ¨™èˆ‡çµ±è¨ˆåˆ†æ

```python
def comprehensive_evaluation():
    """å…¨é¢çš„è©•ä¼°æ¡†æ¶"""
    
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score,
        roc_auc_score, average_precision_score, matthews_corrcoef,
        confusion_matrix, classification_report
    )
    
    def calculate_all_metrics(y_true, y_pred, y_pred_proba=None):
        """è¨ˆç®—æ‰€æœ‰è©•ä¼°æŒ‡æ¨™"""
        
        metrics = {}
        
        # åŸºæœ¬åˆ†é¡æŒ‡æ¨™
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision'] = precision_score(y_true, y_pred)
        metrics['recall'] = recall_score(y_true, y_pred)
        metrics['f1'] = f1_score(y_true, y_pred)
        metrics['mcc'] = matthews_corrcoef(y_true, y_pred)
        
        # æ©Ÿç‡ç›¸é—œæŒ‡æ¨™
        if y_pred_proba is not None:
            metrics['auc'] = roc_auc_score(y_true, y_pred_proba)
            metrics['ap'] = average_precision_score(y_true, y_pred_proba)
        
        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(y_true, y_pred)
        metrics['tn'], metrics['fp'], metrics['fn'], metrics['tp'] = cm.ravel()
        
        # è¨ˆç®—æ›´å¤šæŒ‡æ¨™
        metrics['specificity'] = metrics['tn'] / (metrics['tn'] + metrics['fp'])
        metrics['npv'] = metrics['tn'] / (metrics['tn'] + metrics['fn'])  # è² é æ¸¬å€¼
        metrics['fpr'] = metrics['fp'] / (metrics['fp'] + metrics['tn'])  # å‡é™½æ€§ç‡
        metrics['fnr'] = metrics['fn'] / (metrics['fn'] + metrics['tp'])  # å‡é™°æ€§ç‡
        
        return metrics
    
    def bootstrap_confidence_intervals(y_true, y_pred, y_pred_proba=None, 
                                     n_bootstrap=1000, confidence=0.95):
        """è¨ˆç®—æŒ‡æ¨™çš„ä¿¡è³´å€é–“"""
        
        n_samples = len(y_true)
        bootstrap_metrics = []
        
        for _ in range(n_bootstrap):
            # è‡ªåŠ©æ³•æ¡æ¨£
            indices = np.random.choice(n_samples, n_samples, replace=True)
            y_true_boot = y_true[indices]
            y_pred_boot = y_pred[indices]
            y_pred_proba_boot = y_pred_proba[indices] if y_pred_proba is not None else None
            
            # è¨ˆç®—æŒ‡æ¨™
            metrics = calculate_all_metrics(y_true_boot, y_pred_boot, y_pred_proba_boot)
            bootstrap_metrics.append(metrics)
        
        # è¨ˆç®—ä¿¡è³´å€é–“
        alpha = 1 - confidence
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        confidence_intervals = {}
        for metric in bootstrap_metrics[0].keys():
            values = [m[metric] for m in bootstrap_metrics]
            ci_lower = np.percentile(values, lower_percentile)
            ci_upper = np.percentile(values, upper_percentile)
            confidence_intervals[metric] = (ci_lower, ci_upper)
        
        return confidence_intervals
    
    def statistical_significance_test(results_a, results_b, metric='accuracy'):
        """çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦"""
        
        from scipy import stats
        
        # æå–æŒ‡æ¨™å€¼
        values_a = [r[metric] for r in results_a]
        values_b = [r[metric] for r in results_b]
        
        # é…å° t æª¢é©—
        t_stat, p_value = stats.ttest_rel(values_a, values_b)
        
        # Wilcoxon ç¬¦è™Ÿç§©æª¢é©—ï¼ˆéåƒæ•¸ï¼‰
        w_stat, w_p_value = stats.wilcoxon(values_a, values_b)
        
        return {
            't_test': {'statistic': t_stat, 'p_value': p_value},
            'wilcoxon': {'statistic': w_stat, 'p_value': w_p_value},
            'effect_size': (np.mean(values_a) - np.mean(values_b)) / np.sqrt(
                (np.var(values_a) + np.var(values_b)) / 2
            )
        }
    
    return calculate_all_metrics, bootstrap_confidence_intervals, statistical_significance_test

# å¯¦éš›ä½¿ç”¨ç¯„ä¾‹
eval_functions = comprehensive_evaluation()
calculate_all_metrics, bootstrap_ci, significance_test = eval_functions

# è¼‰å…¥æ¸¬è©¦çµæœ
X_test = ember.read_vectorized_features("test")
y_test = ember.read_labels("test")
model = ember.load_model()

# é æ¸¬
y_pred_proba = model.predict(X_test)
y_pred = (y_pred_proba > 0.5).astype(int)

# ç§»é™¤æœªæ¨™è¨˜æ¨£æœ¬
mask = y_test != -1
y_test_clean = y_test[mask]
y_pred_clean = y_pred[mask]
y_pred_proba_clean = y_pred_proba[mask]

# è¨ˆç®—æŒ‡æ¨™
metrics = calculate_all_metrics(y_test_clean, y_pred_clean, y_pred_proba_clean)
print("è©•ä¼°æŒ‡æ¨™:")
for metric, value in metrics.items():
    print(f"  {metric}: {value:.4f}")

# è¨ˆç®—ä¿¡è³´å€é–“
ci = bootstrap_ci(y_test_clean, y_pred_clean, y_pred_proba_clean)
print("\n95% ä¿¡è³´å€é–“:")
for metric, (lower, upper) in ci.items():
    if metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:
        print(f"  {metric}: [{lower:.4f}, {upper:.4f}]")
```

### 3. å¯¦é©—è¨˜éŒ„å’Œç®¡ç†

```python
class ExperimentTracker:
    """å¯¦é©—è¿½è¹¤å’Œç®¡ç†ç³»çµ±"""
    
    def __init__(self, project_name="ember_experiments"):
        self.project_name = project_name
        self.experiment_id = self.generate_experiment_id()
        self.results_dir = Path(f"experiments/{self.experiment_id}")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.experiment_log = {
            'experiment_id': self.experiment_id,
            'timestamp': datetime.now().isoformat(),
            'config': {},
            'metrics': {},
            'artifacts': []
        }
    
    def generate_experiment_id(self):
        """ç”Ÿæˆå¯¦é©—ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"exp_{timestamp}"
    
    def log_config(self, config):
        """è¨˜éŒ„å¯¦é©—é…ç½®"""
        self.experiment_log['config'] = config
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = self.results_dir / "config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        self.experiment_log['artifacts'].append(str(config_path))
    
    def log_metrics(self, metrics, step=None):
        """è¨˜éŒ„è©•ä¼°æŒ‡æ¨™"""
        if step is not None:
            if 'step_metrics' not in self.experiment_log:
                self.experiment_log['step_metrics'] = {}
            self.experiment_log['step_metrics'][step] = metrics
        else:
            self.experiment_log['metrics'].update(metrics)
    
    def log_model(self, model, model_name="model"):
        """è¨˜éŒ„æ¨¡å‹"""
        import joblib
        
        model_path = self.results_dir / f"{model_name}.pkl"
        joblib.dump(model, model_path)
        
        self.experiment_log['artifacts'].append(str(model_path))
        return model_path
    
    def log_figure(self, fig, name):
        """è¨˜éŒ„åœ–è¡¨"""
        fig_path = self.results_dir / f"{name}.png"
        fig.savefig(fig_path, dpi=300, bbox_inches='tight')
        
        self.experiment_log['artifacts'].append(str(fig_path))
        return fig_path
    
    def log_data(self, data, name, format='npy'):
        """è¨˜éŒ„è³‡æ–™"""
        if format == 'npy':
            data_path = self.results_dir / f"{name}.npy"
            np.save(data_path, data)
        elif format == 'csv':
            data_path = self.results_dir / f"{name}.csv"
            data.to_csv(data_path, index=False)
        elif format == 'json':
            data_path = self.results_dir / f"{name}.json"
            with open(data_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.experiment_log['artifacts'].append(str(data_path))
        return data_path
    
    def save_experiment(self):
        """ä¿å­˜å¯¦é©—è¨˜éŒ„"""
        log_path = self.results_dir / "experiment_log.json"
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(self.experiment_log, f, ensure_ascii=False, indent=2)
        
        return log_path
    
    def load_experiment(self, experiment_id):
        """è¼‰å…¥å¯¦é©—è¨˜éŒ„"""
        log_path = Path(f"experiments/{experiment_id}/experiment_log.json")
        with open(log_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def compare_experiments(self, experiment_ids, metric='accuracy'):
        """æ¯”è¼ƒå¤šå€‹å¯¦é©—"""
        results = []
        
        for exp_id in experiment_ids:
            exp_log = self.load_experiment(exp_id)
            results.append({
                'experiment_id': exp_id,
                'timestamp': exp_log['timestamp'],
                'metric_value': exp_log['metrics'].get(metric, None),
                'config': exp_log['config']
            })
        
        # æ’åº
        results.sort(key=lambda x: x['metric_value'] or 0, reverse=True)
        
        return results

# ä½¿ç”¨ç¯„ä¾‹
tracker = ExperimentTracker("ember_malware_detection")

# è¨˜éŒ„é…ç½®
config = {
    "model": "lightgbm",
    "features": "all_2351",
    "preprocessing": "robust_scaler",
    "feature_selection": "top_1000"
}
tracker.log_config(config)

# è¨˜éŒ„æŒ‡æ¨™
metrics = {
    "accuracy": 0.9234,
    "precision": 0.9187,
    "recall": 0.9289,
    "f1": 0.9238,
    "auc": 0.9756
}
tracker.log_metrics(metrics)

# è¨˜éŒ„æ¨¡å‹å’Œçµæœ
# tracker.log_model(trained_model, "lightgbm_model")
# tracker.log_figure(confusion_matrix_plot, "confusion_matrix")

# ä¿å­˜å¯¦é©—
log_path = tracker.save_experiment()
print(f"å¯¦é©—è¨˜éŒ„ä¿å­˜è‡³: {log_path}")
```

## æŠ€è¡“åƒè€ƒ

### æ ¸å¿ƒæŠ€è¡“åƒæ•¸

#### 1. æ¨¡å‹è¶…åƒæ•¸é…ç½®

```python
# LightGBM æœ€ä½³åƒæ•¸
LIGHTGBM_PARAMS = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 1024,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'max_depth': -1,
    'min_data_in_leaf': 20,
    'lambda_l1': 0.1,
    'lambda_l2': 0.1
}

# ç‰¹å¾µé¸æ“‡åƒæ•¸
FEATURE_SELECTION = {
    'method': 'mutual_info_classif',
    'k_best': 1000,
    'variance_threshold': 0.01
}
```

#### 2. æ€§èƒ½åŸºæº–

```python
# Ember å®˜æ–¹åŸºæº–çµæœ
EMBER_BENCHMARKS = {
    'LightGBM': {
        'accuracy': 0.8958,
        'precision': 0.8951,
        'recall': 0.8965,
        'f1_score': 0.8958,
        'auc_score': 0.9761
    },
    'RandomForest': {
        'accuracy': 0.8834,
        'precision': 0.8798,
        'recall': 0.8871,
        'f1_score': 0.8834,
        'auc_score': 0.9572
    }
}
```

#### 3. æŠ€è¡“é™åˆ¶èˆ‡æ³¨æ„äº‹é …

- **è³‡æ–™ä¾è³´æ€§**ï¼šæ¨¡å‹æ•ˆèƒ½å—è¨“ç·´è³‡æ–™æ™‚é–“ç¯„åœå½±éŸ¿
- **ç‰¹å¾µæ¼‚ç§»**ï¼šæƒ¡æ„è»Ÿé«”æ¼”åŒ–å¯èƒ½å°è‡´ç‰¹å¾µåˆ†ä½ˆè®ŠåŒ–  
- **å°æŠ—æ€§æ”»æ“Š**ï¼šéœ€è¦å®šæœŸè©•ä¼°æ¨¡å‹å°æŠ—æ”»æ“Šçš„æŠ—æ€§
- **è¨ˆç®—è³‡æº**ï¼šå®Œæ•´è¨“ç·´éœ€è¦16GB+ RAMå’Œå¤šæ ¸CPU

### æ ¸å¿ƒæƒ¡æ„è»Ÿé«”åˆ†ææ¦‚å¿µ

#### éœæ…‹åˆ†æåŸºç¤æŠ€è¡“

```python
# æƒ¡æ„è»Ÿé«”ç‰¹å¾µè­˜åˆ¥
malware_indicators = {
    'packed_executables': {
        'entropy_threshold': 7.0,  # é«˜ç†µå€¼è¡¨ç¤ºå¯èƒ½è¢«æ‰“åŒ…
        'section_names': ['.upx', '.aspack', '.petite'],
        'import_anomalies': 'few_imports_high_entropy'
    },
    'suspicious_apis': {
        'process_manipulation': ['CreateProcess', 'VirtualAlloc', 'WriteProcessMemory'],
        'file_operations': ['CreateFile', 'WriteFile', 'MoveFile', 'DeleteFile'],
        'registry_access': ['RegCreateKey', 'RegSetValue', 'RegDeleteKey'],
        'network_activity': ['socket', 'connect', 'send', 'recv', 'InternetOpen'],
        'anti_analysis': ['IsDebuggerPresent', 'GetTickCount', 'QueryPerformanceCounter']
    },
    'pe_anomalies': {
        'unusual_entry_point': 'entry_point_in_writable_section',
        'section_characteristics': 'executable_writable_sections',
        'timestamp_anomalies': 'future_or_null_timestamps'
    }
}
```

#### æƒ¡æ„è»Ÿé«”å®¶æ—ç‰¹å¾µæ¨¡å¼

```python
# å¸¸è¦‹æƒ¡æ„è»Ÿé«”å®¶æ—çš„ Ember ç‰¹å¾µæ¨¡å¼
family_patterns = {
    'trojans': {
        'high_import_features': ['kernel32.dll', 'advapi32.dll', 'user32.dll'],
        'suspicious_strings': ['\\System32\\', 'SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run'],
        'section_entropy': 'mixed_high_low_entropy'
    },
    'ransomware': {
        'crypto_apis': ['CryptGenKey', 'CryptEncrypt', 'CryptAcquireContext'],
        'file_extensions': ['.docx', '.pdf', '.jpg', '.mp3'],
        'string_patterns': ['your files have been encrypted', 'bitcoin']
    },
    'backdoors': {
        'network_apis': ['WSAStartup', 'socket', 'bind', 'listen'],
        'persistence_mechanisms': ['CreateService', 'RegSetValue'],
        'process_apis': ['CreateProcess', 'ShellExecute']
    },
    'info_stealers': {
        'browser_paths': ['\\Chrome\\', '\\Firefox\\', '\\Edge\\'],
        'credential_apis': ['LsaEnumerateLogonSessions', 'CredEnumerate'],
        'file_search': ['FindFirstFile', 'FindNextFile']
    }
}
```

#### é€²éšåˆ†ææŠ€è¡“

```python
def advanced_malware_analysis(ember_features, pe_file_path):
    """é€²éšæƒ¡æ„è»Ÿé«”åˆ†ææŠ€è¡“"""
    
    import pefile
    import math
    
    # 1. PE çµæ§‹åˆ†æ
    pe = pefile.PE(pe_file_path)
    
    # æª¢æŸ¥æ‰“åŒ…è·¡è±¡
    def detect_packing():
        packing_indicators = {
            'high_entropy_sections': [],
            'suspicious_section_names': [],
            'import_anomalies': False
        }
        
        for section in pe.sections:
            entropy = section.get_entropy()
            section_name = section.Name.decode().strip('\x00')
            
            if entropy > 7.0:
                packing_indicators['high_entropy_sections'].append({
                    'name': section_name,
                    'entropy': entropy,
                    'size': section.SizeOfRawData
                })
            
            # æª¢æŸ¥å·²çŸ¥æ‰“åŒ…ç¨‹å¼çš„ç¯€å€åç¨±
            known_packers = ['UPX', 'ASPack', 'PEtite', 'MPRESS']
            if any(packer.lower() in section_name.lower() for packer in known_packers):
                packing_indicators['suspicious_section_names'].append(section_name)
        
        # æª¢æŸ¥åŒ¯å…¥è¡¨ç•°å¸¸ï¼ˆæ‰“åŒ…å¾ŒåŒ¯å…¥å‡½æ•¸é€šå¸¸å¾ˆå°‘ï¼‰
        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            import_count = sum(len(entry.imports) for entry in pe.DIRECTORY_ENTRY_IMPORT)
            if import_count < 10:  # æ­£å¸¸ç¨‹å¼é€šå¸¸æœ‰æ›´å¤šåŒ¯å…¥
                packing_indicators['import_anomalies'] = True
        
        return packing_indicators
    
    # 2. è¡Œç‚ºæ¨¡å¼åˆ†æ
    def analyze_behavior_patterns():
        # åŸºæ–¼ Ember åŒ¯å…¥ç‰¹å¾µåˆ†æè¡Œç‚º
        import_features = ember_features[72:1352]  # åŒ¯å…¥ç‰¹å¾µç´¢å¼•ç¯„åœ
        
        behavior_score = {
            'file_manipulation': 0,
            'registry_access': 0,
            'network_activity': 0,
            'process_control': 0,
            'anti_analysis': 0
        }
        
        # é€™è£¡éœ€è¦æ˜ å°„ Ember ç‰¹å¾µç´¢å¼•åˆ°å…·é«” API
        # å¯¦éš›å¯¦ä½œéœ€è¦ Ember çš„ç‰¹å¾µåç¨±æ˜ å°„è¡¨
        
        return behavior_score
    
    # 3. çµ±è¨ˆç•°å¸¸æª¢æ¸¬
    def statistical_anomaly_detection():
        anomalies = {
            'feature_outliers': [],
            'suspicious_patterns': []
        }
        
        # æª¢æŸ¥ç‰¹å¾µå€¼ç•°å¸¸
        for i, feature_value in enumerate(ember_features):
            if feature_value > 10:  # å‡è¨­æ­£å¸¸å€¼ç¯„åœ
                anomalies['feature_outliers'].append({
                    'feature_index': i,
                    'value': feature_value,
                    'category': get_feature_category(i)
                })
        
        return anomalies
    
    # 4. æ™‚é–“åºåˆ—åˆ†æï¼ˆå¦‚æœæœ‰æ™‚é–“æˆ³è³‡è¨Šï¼‰
    def temporal_analysis():
        if hasattr(pe, 'FILE_HEADER'):
            timestamp = pe.FILE_HEADER.TimeDateStamp
            
            # æª¢æŸ¥æ™‚é–“æˆ³ç•°å¸¸
            from datetime import datetime
            compile_time = datetime.fromtimestamp(timestamp)
            current_time = datetime.now()
            
            temporal_indicators = {
                'compile_time': compile_time,
                'is_future': compile_time > current_time,
                'is_null': timestamp == 0,
                'is_suspicious': timestamp < 946684800  # 2000å¹´ä¹‹å‰
            }
            
            return temporal_indicators
        return None
    
    # åŸ·è¡Œæ‰€æœ‰åˆ†æ
    analysis_results = {
        'packing_analysis': detect_packing(),
        'behavior_patterns': analyze_behavior_patterns(),
        'statistical_anomalies': statistical_anomaly_detection(),
        'temporal_analysis': temporal_analysis()
    }
    
    return analysis_results

def get_feature_category(feature_index):
    """æ ¹æ“šç‰¹å¾µç´¢å¼•è¿”å›ç‰¹å¾µé¡åˆ¥"""
    if feature_index < 10:
        return 'general'
    elif feature_index < 72:
        return 'header'
    elif feature_index < 1352:
        return 'imports'
    elif feature_index < 1480:
        return 'exports'
    elif feature_index < 1735:
        return 'sections'
    elif feature_index < 1991:
        return 'byte_histogram'
    elif feature_index < 2247:
        return 'byte_entropy'
    else:
        return 'strings'
```

---

**æŠ€è¡“è²æ˜**: æœ¬æŠ€è¡“æŒ‡å—å°ˆæ³¨æ–¼ Ember è³‡æ–™é›†çš„é˜²ç¦¦æ€§æƒ¡æ„è»Ÿé«”åˆ†ææŠ€è¡“å¯¦ä½œã€‚